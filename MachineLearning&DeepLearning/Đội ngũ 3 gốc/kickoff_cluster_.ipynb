{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn có thể nhập 2 lệnh đó trực tiếp vào bất kỳ một Python Interpreter nào (ví dụ như IDLE, Jupyter Notebook) hoặc đưa chúng vào script Python và chạy script đó bằng cách gõ `python <tên_file>.py` trong terminal (trong đó `<tên_file>` là tên của file chứa script Python đó). \n",
    "\n",
    "Nhưng để giải quyết lỗi trực tiếp, bạn có thể mở Python Interpreter bằng cách gõ `python` trong Terminal và sau đó thực hiện nhập hai câu lệnh:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "Sau khi chạy xong, bạn có thể thoát khỏi Python Interpreter bằng cách gõ lệnh `exit()`.\n",
    "\n",
    "Khi bạn tải xuống `stopwords` bằng lệnh `nltk.download('stopwords')`, NLTK chỉ tải về các stopword cho tiếng Anh và không bao gồm stopwords cho tiếng Việt. Bạn cần thực hiện các bước trong câu trả lời trước của tôi để có thể sử dụng stopwords tiếng Việt.\n",
    "\n",
    "Lỗi này cho thấy `stopwords` tiếng Việt chưa được tải về trên máy tính của bạn thông qua Natural Language Toolkit (nltk). Bạn có thể sử dụng lệnh `nltk.download('stopwords')` để tải các tập tin cần thiết xuống máy tính của bạn. Tuy nhiên, `stopwords` tiếng Việt không có trong danh sách mặc định của nltk, vì vậy bạn cần chỉ định tên gói riêng.\n",
    "\n",
    "Bạn có thể làm theo hướng dẫn sau:\n",
    "\n",
    "1. Tải các tập tin `vietnamese_stopwords.zip` từ https://github.com/stopwords/vietnamese-stopwords và giải nén nó.\n",
    "2. Sao chép tệp `vietnamese.txt` trong thư mục `vi` để vào thư mục stopwords của nltk: `C:\\Users\\YourUserName\\AppData\\Roaming\\nltk_data\\corpora\\stopwords\\vi` (Hãy đảm bảo rằng bạn đã thay thế `YourUserName` bằng tên đăng nhập của bạn trên máy tính).\n",
    "3. Giờ đây bạn có thể sử dụng stopwords tiếng Việt bằng cách sử dụng khối lệnh sau:\n",
    "```python\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('vi'))\n",
    "```\n",
    "\n",
    "Lưu ý rằng bạn cũng cần tải xuống `punkt`, một bộ tokenizer của nltk, để sử dụng `stopwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.037*\",\" + 0.037*\"xây\" + 0.035*\"học\" + 0.035*\"phát\" + 0.034*\"dựng\" + 0.028*\"triển\" + 0.027*\"doanh\" + 0.023*\"nối\" + 0.023*\"đội\" + 0.021*\"nghiệp\" + 0.017*\"kinh\" + 0.015*\"ko\" + 0.014*\"tổ\" + 0.014*\"chức\" + 0.014*\"gia\"')\n",
      "(1, '0.091*\"đội\" + 0.075*\"kết\" + 0.067*\"nhân\" + 0.045*\"ngũ\" + 0.039*\",\" + 0.037*\"đoàn\" + 0.028*\"gắn\" + 0.021*\"hiệu\" + 0.020*\"xây\" + 0.018*\"lực\" + 0.017*\"viên\" + 0.017*\"dựng\" + 0.015*\"động\" + 0.015*\"tuyển\" + 0.014*\"quản\"')\n",
      "(2, '0.041*\".\" + 0.029*\"đạo\" + 0.025*\"lãnh\" + 0.020*\"mong\" + 0.019*\"quản\" + 0.017*\"hạnh\" + 0.017*\"phúc\" + 0.014*\"dụng\" + 0.013*\"gốc\" + 0.013*\"đồng\" + 0.013*\"kiến\" + 0.013*\"vấn\" + 0.013*\"phù\" + 0.013*\"hợp\" + 0.012*\"hàng\"')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Tải stopwords và punkt từ NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load dữ liệu từ dataframe và loại bỏ các hàng chứa NaN\n",
    "df = pd.read_excel('kickoff_data.xlsx')\n",
    "df.dropna(subset=['Vấn đề mà doanh nghiệp, tổ chức của bạn đang gặp phải là gì? Hãy chia sẻ thật cụ thể'], inplace=True)\n",
    "\n",
    "# Thay thế các giá trị NaN bằng rỗng ('') trên dataframe gốc\n",
    "df['Vấn đề mà doanh nghiệp, tổ chức của bạn đang gặp phải là gì? Hãy chia sẻ thật cụ thể'].fillna('', inplace=True)\n",
    "\n",
    "# Tiếp tục xử lý dữ liệu\n",
    "problems_column_data = df['Vấn đề mà doanh nghiệp, tổ chức của bạn đang gặp phải là gì? Hãy chia sẻ thật cụ thể']\n",
    "\n",
    "# Loại bỏ stopwords và tạo dictionary\n",
    "stop_words = set(stopwords.words('vietnamese'))\n",
    "processed_texts = [word_tokenize(text.lower()) for text in problems_column_data]\n",
    "cleaned_texts = [[word for word in tokens if not word in stop_words] for tokens in processed_texts]\n",
    "dictionary = corpora.Dictionary(cleaned_texts)\n",
    "doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in cleaned_texts]\n",
    "\n",
    "# Mô hình LDA (mã hóa Bow): Ánh xạ các từ thành vector trong không gian vector chủ đề\n",
    "lda_model = models.LdaModel(doc_term_matrix, num_topics=3, id2word=dictionary, passes=50)\n",
    "topics = lda_model.show_topics(num_topics=3, num_words=15)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "334\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_texts))\n",
    "print(len(dictionary))\n",
    "print(len(set()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (255,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Áp dụng thuật toán K-means với n_clusters=3\u001b[39;00m\n\u001b[0;32m      4\u001b[0m kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m kmeans\u001b[39m.\u001b[39;49mfit(doc_term_matrix)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Nhãn của từng câu -  tương ứng với một nhóm phân cụm\u001b[39;00m\n\u001b[0;32m      8\u001b[0m labels \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39mlabels_\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1417\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m \n\u001b[0;32m   1392\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m-> 1417\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1418\u001b[0m     X,\n\u001b[0;32m   1419\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1420\u001b[0m     dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[0;32m   1421\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1422\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_x,\n\u001b[0;32m   1423\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1424\u001b[0m )\n\u001b[0;32m   1426\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m   1428\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (255,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Áp dụng thuật toán K-means với n_clusters=3\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(doc_term_matrix)\n",
    "\n",
    "# Nhãn của từng câu -  tương ứng với một nhóm phân cụm\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# In ra các câu trong từng nhóm phân cụm\n",
    "for i in range(3):\n",
    "    print(f\"Nhóm {i+1}:\")\n",
    "    for j, text in enumerate(problems_column_data):\n",
    "        if labels[j] == i:\n",
    "            print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
